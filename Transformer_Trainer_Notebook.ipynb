{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadhiin/Attention-Is-All-You-Need/blob/main/Transformer_Trainer_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYGVziz50tu"
      },
      "source": [
        "- Remove non alphanumeric characters for simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7zNJNCRxJE-",
        "outputId": "36269076-73aa-4693-c4fc-d85048c6cd53"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JYEzlV3RxYK_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "\n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                d_model,\n",
        "                ffn_hidden,\n",
        "                num_heads,\n",
        "                drop_prob,\n",
        "                num_layers,\n",
        "                max_sequence_length,\n",
        "                bn_vocab_size,\n",
        "                english_to_index,\n",
        "                kannada_to_index,\n",
        "                START_TOKEN,\n",
        "                END_TOKEN,\n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, bn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "# from transformer import Transformer # A local file transformer.py\n",
        "import torch\n",
        "import numpy as npp\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6TApzOj5xCwR"
      },
      "outputs": [],
      "source": [
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "bangla_vocabulary = [\n",
        "    START_TOKEN, ' ', '!', '\"', '#', '€', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
        "    ',', ';', '-', '.', '/', ':', '<', '=', '>', '?', '@',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯'\n",
        "    'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ',\n",
        "    'ক', 'খ', 'গ', 'ঘ', 'ঙ',\n",
        "    'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
        "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ',\n",
        "    'ত', 'থ', 'দ', 'ধ', 'ন',\n",
        "    'প', 'ফ', 'ব', 'ভ', 'ম',\n",
        "    'য', 'র', 'ল', 'শ', 'ষ',\n",
        "    'স', 'হ', 'ড়', 'ঢ', 'য়', 'ঃ', 'ং', '্',\n",
        "    'ো', 'ি', 'ৌ', 'ৗ', 'ৎ', 'ী', 'ু', 'ূ', 'ৃ', 'ৄ', 'ে', 'ৈ', '।',\n",
        "    'ঁ', 'া', '্র', '্ন',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gA8ESmCrNoc7"
      },
      "outputs": [],
      "source": [
        "index_to_bangla = {k:v for k,v in enumerate(bangla_vocabulary)}\n",
        "bangla_to_index = {v:k for k,v in enumerate(bangla_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sOQe0xI5xJFG"
      },
      "outputs": [],
      "source": [
        "bn_txt_save_path = '/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/bn_data.txt'\n",
        "en_txt_save_path = '/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/en_data.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe0h6mSvxy5l",
        "outputId": "e45562e9-b5e7-4464-995c-fbc007fd8db0"
      },
      "outputs": [],
      "source": [
        "# !wget 'https://raw.githubusercontent.com/sadhiin/Attention-Is-All-You-Need/refs/heads/main/data/bn_data.txt'\n",
        "# !wget 'https://raw.githubusercontent.com/sadhiin/Attention-Is-All-You-Need/refs/heads/main/data/en_data.txt'\n",
        "\n",
        "# bn_txt_save_path = 'bn_data.txt'\n",
        "# en_txt_save_path = 'en_data.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9SYGjRdoxRg-"
      },
      "outputs": [],
      "source": [
        "with open(en_txt_save_path, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "\n",
        "with open(bn_txt_save_path, 'r') as file:\n",
        "    bangla_sentences = file.readlines()\n",
        "\n",
        "# # Limit Number of sentences\n",
        "# TOTAL_SENTENCES = 200000\n",
        "# english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "# bangla_sentences = bangla_sentences[:TOTAL_SENTENCES]\n",
        "\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "bangla_sentences = [sentence.rstrip('\\n') for sentence in bangla_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUB-BkgFxXfM",
        "outputId": "86699d1a-52c8-4de4-b32c-d2486d706272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"janet’s ducks lay 16 eggs per day. she eats three for breakfast every morning and bakes muffins for her friends every day with four. she sells the remainder at the farmers' market daily for $2 per fresh duck egg. how much in dollars does she make every day at the farmers' market?\",\n",
              " 'a robe takes 2 bolts of blue fiber and half that much white fiber.  how many bolts in total does it take?',\n",
              " 'josh decides to try flipping a house.  he buys a house for $80,000 and then puts in $50,000 in repairs.  this increased the value of the house by 150%.  how much profit did he make?']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OT-aznAxc5U",
        "outputId": "22876290-9dd6-486c-e164-6479914c71ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['জেনেটের হাঁসগুলি প্রতিদিন 16টি করে ডিম পাড়ে। তিনি প্রতিদিন প্রাতরাশে তিনটি করে ডিম খান এবং বন্ধুদের জন্য প্রতিদিন চারটি ডিম দিয়ে মাফিন তৈরি করেন। অবশিষ্ট হাঁসের ডিমগুলি তিনি প্রতিদিন কৃষকদের বাজারে প্রতি ডিম $2 দরে বিক্রয় করেন। তিনি কৃষকদের বাজারে প্রতিদিন কত ডলার উপার্জন করেন?',\n",
              " 'একটি পোশাক তৈরি করতে 2 রিল নীল সুতো এবং তার অর্ধেক সাদা সুতো লাগে। এটি তৈরি করতে মোট কতগুলি রিল লাগবে?',\n",
              " 'জোশ একটি বাড়ি ফ্লিপ করার সিদ্ধান্ত নিয়েছেন। তিনি $80,000-এ একটি বাড়ি ক্রয় করলেন এবং $50,000 দিয়ে মেরামত করলেন। এর ফলে বাড়িটির মূল্য 150% বৃদ্ধি পেল। তিনি কত ডলার লাভ করলেন?']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bangla_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VAutsTxlaR",
        "outputId": "7cbdb92f-8a17-4764-ee13-a9ac8ab2d6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length bangla: 477.06\n",
            "97th percentile length English: 468.18\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length bangla: {np.percentile([len(x) for x in bangla_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9ezqvaxl4b",
        "outputId": "1101ceef-c9ef-468f-92da-959d3358846c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 250\n",
            "Number of valid sentences: 4\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(bangla_sentences)):\n",
        "    bangla_sentence, english_sentence = bangla_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(bangla_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(bangla_sentence, bangla_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(bangla_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o80QDn4CxsV7"
      },
      "outputs": [],
      "source": [
        "bangla_sentences = [bangla_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xhLztQiLIQ",
        "outputId": "46b516b2-3860-47fe-e506-cd225b79d23b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['গ্রেচেনের কাছে 110টি মুদ্রা আছে। রৌপ্য মুদ্রার থেকে 30টি বেশি স্বর্ণ মুদ্রা আছে। গ্রেচেনের কাছে কতগুলি স্বর্ণ মুদ্রা আছে?',\n",
              " 'পাঞ্চো একদিনে 20 মাইল হাঁটেন। সপ্তাহান্তে ব্যতিরেকে তিনি 10 মাইল হাঁটেন। এক সপ্তাহে তিনি কত মাইল হাঁটেন?',\n",
              " 'জন 3 ঘণ্টা ধরে পিচ সংগ্রহ করেন। তিনি এক মিনিটে 2টি পিচ সংগ্রহ করতে পারেন। তিনি কতগুলি পিচ সংগ্রহ করেন?']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bangla_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xqOFnclmyxAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 2\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "bn_vocab_size = len(bangla_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          bn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          bangla_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2hYQk9yxX0",
        "outputId": "0fcd103b-d71f-4de2-9c44-6b0bc429f55a"
      },
      "outputs": [],
      "source": [
        "# transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "asUJX-STy7fg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, bangla_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.bangla_sentences = bangla_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.bangla_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-auNWjkdzDge"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentences, bangla_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roH2A4m4zF4z",
        "outputId": "f9035a01-5e43-43fa-8e16-3c82bc94ce9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeHNlzozIGF",
        "outputId": "caea8dc6-d567-46e7-fd8d-12dbce4441e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('pancho walks 20 miles a day. except on weekends when he walks 10 miles. how many miles does he walk in a week?',\n",
              " 'পাঞ্চো একদিনে 20 মাইল হাঁটেন। সপ্তাহান্তে ব্যতিরেকে তিনি 10 মাইল হাঁটেন। এক সপ্তাহে তিনি কত মাইল হাঁটেন?')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5YDttjQ0zMrv"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "9EnjHKB1zM8Y",
        "outputId": "c10554f6-48a1-4aef-d11e-e7a07344e213"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'gretchen has 110 coins. there are 30 more gold coins than silver coins. how many gold coins does gretchen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">have?'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'pancho walks 20 miles a day. except on weekends when he walks 10 miles. how many miles does he walk in a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">week?'</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'গ্রেচেনের কাছে 110টি মুদ্রা আছে। রৌপ্য মুদ্রার থেকে 30টি বেশি স্বর্ণ মুদ্রা আছে। গ্রেচেনের কাছে কতগুলি স্বর্ণ মুদ্রা আছে?'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'পাঞ্চো একদিনে 20 মাইল হাঁটেন। সপ্তাহান্তে ব্যতিরেকে তিনি 10 মাইল হাঁটেন। এক সপ্তাহে তিনি কত মাইল হাঁটেন?'</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'gretchen has 110 coins. there are 30 more gold coins than silver coins. how many gold coins does gretchen \u001b[0m\n",
              "\u001b[32mhave?'\u001b[0m,\n",
              "        \u001b[32m'pancho walks 20 miles a day. except on weekends when he walks 10 miles. how many miles does he walk in a \u001b[0m\n",
              "\u001b[32mweek?'\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'গ্রেচেনের কাছে 110টি মুদ্রা আছে। রৌপ্য মুদ্রার থেকে 30টি বেশি স্বর্ণ মুদ্রা আছে। গ্রেচেনের কাছে কতগুলি স্বর্ণ মুদ্রা আছে?'\u001b[0m,\n",
              "        \u001b[32m'পাঞ্চো একদিনে 20 মাইল হাঁটেন। সপ্তাহান্তে ব্যতিরেকে তিনি 10 মাইল হাঁটেন। এক সপ্তাহে তিনি কত মাইল হাঁটেন?'\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'john collects peaches for 3 hours.  he can collect 2 peaches a minute.  how many peaches does he </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">collect?'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'james has 6 more candies than robert. john has twice as many candies as robert. if john has 54 candies, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">how many more candies does john have than james?'</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'জন 3 ঘণ্টা ধরে পিচ সংগ্রহ করেন। তিনি এক মিনিটে 2টি পিচ সংগ্রহ করতে পারেন। তিনি কতগুলি পিচ সংগ্রহ করেন?'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'জেমসের কাছে রবার্টের থেকে 6টি ক্যান্ডি বেশি আছে। জনের কাছে রবার্টের থেকে দুইগুণ বেশি ক্যান্ডি আছে। জনের কাছে যদি 54টি ক্যান্ডি থাকে তাহলে জনের কাছে জেমসের থেকে</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">কতগুলি ক্যান্ডি বেশি আছে?'</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'john collects peaches for 3 hours.  he can collect 2 peaches a minute.  how many peaches does he \u001b[0m\n",
              "\u001b[32mcollect?'\u001b[0m,\n",
              "        \u001b[32m'james has 6 more candies than robert. john has twice as many candies as robert. if john has 54 candies, \u001b[0m\n",
              "\u001b[32mhow many more candies does john have than james?'\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'জন 3 ঘণ্টা ধরে পিচ সংগ্রহ করেন। তিনি এক মিনিটে 2টি পিচ সংগ্রহ করতে পারেন। তিনি কতগুলি পিচ সংগ্রহ করেন?'\u001b[0m,\n",
              "        \u001b[32m'জেমসের কাছে রবার্টের থেকে 6টি ক্যান্ডি বেশি আছে। জনের কাছে রবার্টের থেকে দুইগুণ বেশি ক্যান্ডি আছে। জনের কাছে যদি 54টি ক্যান্ডি থাকে তাহলে জনের কাছে জেমসের থেকে\u001b[0m\n",
              "\u001b[32mকতগুলি ক্যান্ডি বেশি আছে?'\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from rich import print\n",
        "# for batch_num, batch in enumerate(iterator):\n",
        "#     print(batch)\n",
        "#     if batch_num > 3:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnanjzqtzQi8",
        "outputId": "99508a40-a24c-4170-abc4-fdc68afffdcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=bangla_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_saWU5QmVem2"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, bn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, bn_sentence_length = len(eng_batch[idx]), len(bn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      bn_chars_to_padding_mask = np.arange(bn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, bn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, bn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, bn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "ju59VDGLuOqf",
        "outputId": "9a622073-dba3-4206-a194-931bf46434c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CUDA is available!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "CUDA is available!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current device: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Current device: \u001b[1;36m0\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device name: Tesla T4\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Device name: Tesla T4\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of GPUs: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Number of GPUs: \u001b[1;36m1\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Allocated memory: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Allocated memory: \u001b[1;36m0\u001b[0m bytes\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_30957/1467383845.py:17: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print(f\"Cached memory: {torch.cuda.memory_cached()} bytes\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cached memory: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Cached memory: \u001b[1;36m0\u001b[0m bytes\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Epoch \u001b[1;36m0\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [169,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [261,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [262,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, bn_batch)\n\u001b[1;32m     40\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m bn_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(bn_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[1;32m     52\u001b[0m     bn_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bn_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     53\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[2], line 302\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    292\u001b[0m             x,\n\u001b[1;32m    293\u001b[0m             y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m             dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[1;32m    300\u001b[0m             dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, encoder_self_attention_mask, start_token\u001b[38;5;241m=\u001b[39menc_start_token, end_token\u001b[38;5;241m=\u001b[39menc_end_token)\n\u001b[0;32m--> 302\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[2], line 265\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n\u001b[0;32m--> 265\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, y, self_attention_mask, cross_attention_mask)\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[0;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_tokenize(x, start_token, end_token)\n\u001b[0;32m---> 71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x \u001b[38;5;241m+\u001b[39m pos)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs: {num_gpus}\")\n",
        "\n",
        "\n",
        "gpu_index=0\n",
        "device = torch.device(f'cuda:{gpu_index}')  # e.g., 'cuda:0', 'cuda:1'\n",
        "# Check memory allocation\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Allocated memory: {torch.cuda.memory_allocated()} bytes\")\n",
        "    print(f\"Cached memory: {torch.cuda.memory_cached()} bytes\")\n",
        "\n",
        "\n",
        "\n",
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        transformer.train()\n",
        "        eng_batch, bn_batch = batch\n",
        "\n",
        "        # assert torch.all(torch.isfinite(eng_batch)), \"English batch Input contains NaN or Inf values!\"\n",
        "        # assert torch.all(torch.isfinite(bn_batch)), \"Bangla batch Input contains NaN or Inf values!\"\n",
        "        del batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, bn_batch)\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        bn_predictions = transformer(eng_batch,\n",
        "                                     bn_batch,\n",
        "                                     encoder_self_attention_mask.to(device),\n",
        "                                     decoder_self_attention_mask.to(device),\n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(bn_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            bn_predictions.view(-1, bn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == bangla_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"bangla Translation: {bn_batch[0]}\")\n",
        "            bn_sentence_predicted = torch.argmax(bn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in bn_sentence_predicted:\n",
        "              if idx == bangla_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_bangla[idx.item()]\n",
        "            print(f\"bangla Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            bn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, bn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          bn_sentence,\n",
        "                                          encoder_self_attention_mask.to(device),\n",
        "                                          decoder_self_attention_mask.to(device),\n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_bangla[next_token_index]\n",
        "                bn_sentence = (bn_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {bn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  bn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, bn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              bn_sentence,\n",
        "                              encoder_self_attention_mask.to(device),\n",
        "                              decoder_self_attention_mask.to(device),\n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_bangla[next_token_index]\n",
        "    bn_sentence = (bn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return bn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDVH_YsxlK6q"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9yfawBnul0W"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpdYBk5-urcQ"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕುಟುಂಬದ ಹೆಸರು<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJuJKHqFldW3"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಯಾವುದೇ ಕಾರಣಗಳು ಇಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏಕೆ ಕಾರಣ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB6TEJfGlkRT"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQwDbuWBlmmA"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\".\n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ifeV4bGluIj"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RB5DUBEl1kD"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdeJ9CvMn5LM"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GFeyzrg1fIZ"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0upygLS-sXcO"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSrsqEGmtcl2"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ನಾನು ಕೊಡುತ್ತೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so.\n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvBs5l0wxJFj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
