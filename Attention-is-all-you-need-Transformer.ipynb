{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"pwiioRNweEer"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qL1c5UKoeEa2"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        # Ensure that the model dimension (d_model) is divisible by the number of heads\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        # Initialize dimensions\n","        self.d_model = d_model # Model's dimension\n","        self.num_heads = num_heads # Number of attention heads\n","        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n","\n","        # Linear layers for transforming inputs\n","        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n","        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n","        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n","        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        # Calculate attention scores\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","\n","        # Softmax is applied to obtain attention probabilities\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","\n","        # Multiply by values to obtain the final output\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def split_heads(self, x):\n","        # Reshape the input to have num_heads for multi-head attention\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        # Combine the multiple heads back to original shape\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        # Apply linear transformations and split heads\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        # Perform scaled dot-product attention\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","\n","        # Combine heads and apply output transformation\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output\n","\n","\n","class PositionWiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_seq_length, d_model)\n","        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        attn_output = self.self_attn(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        attn_output = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n","        x = self.norm2(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm3(x + self.dropout(ff_output))\n","        return x\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n","        super(Transformer, self).__init__()\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def generate_mask(self, src, tgt):\n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n","        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n","        seq_length = tgt.size(1)\n","        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(tgt.device)\n","        tgt_mask = tgt_mask & nopeak_mask\n","        return src_mask, tgt_mask\n","\n","    def forward(self, src, tgt):\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","\n","        enc_output = src_embedded\n","        for enc_layer in self.encoder_layers:\n","            enc_output = enc_layer(enc_output, src_mask)\n","\n","        dec_output = tgt_embedded\n","        for dec_layer in self.decoder_layers:\n","            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n","\n","        output = self.fc(dec_output)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"tmfpRT8hd8xh"},"source":["# Pipeline for transformer\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9L9C-tDmd4-z"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import json\n","import math\n","from collections import Counter\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"s1cnMcVseFxx"},"outputs":[],"source":["# =============================\n","# Vocabulary and Tokenization\n","# =============================\n","\n","# def build_vocab(file, min_freq=2, special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]):\n","#     with open(file, encoding=\"utf-8\") as f:\n","#         words = Counter(word for line in f for word in line.strip().split())\n","#     vocab = {token: idx for idx, token in enumerate(special_tokens)}\n","#     for word, freq in words.items():\n","#         if freq >= min_freq and word not in vocab:\n","#             vocab[word] = len(vocab)\n","#     return vocab\n","\n","# def sentence_to_ids(sentence, vocab, max_len=50):\n","#     tokens = sentence.split()\n","#     ids = [vocab.get(\"<sos>\")] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens[:max_len]] + [vocab.get(\"<eos>\")]\n","#     return ids + [vocab.get(\"<pad>\")] * (max_len - len(ids))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bangla IDs: [2, 1, 1, 4, 755, 806, 6, 7, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","English IDs: [2, 1, 4, 1, 5, 6, 14, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["from collections import Counter\n","\n","def build_vocab(file, min_freq=2, special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]):\n","    with open(file, encoding=\"utf-8\") as f:\n","        words = Counter(word for line in f for word in line.strip().split())\n","    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n","    for word, freq in words.items():\n","        if freq >= min_freq and word not in vocab:\n","            vocab[word] = len(vocab)\n","    return vocab\n","\n","def sentence_to_ids(sentence, vocab, max_len=50):\n","    tokens = sentence.split()\n","    ids = [vocab.get(\"<sos>\")] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens[:max_len]] + [vocab.get(\"<eos>\")]\n","    return ids + [vocab.get(\"<pad>\")] * (max_len - len(ids))\n","\n","# Build vocabularies\n","bangla_vocab = build_vocab(\"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/bn_data.txt\")\n","english_vocab = build_vocab(\"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/en_data.txt\")\n","\n","# Example sentences\n","bangla_sentence = \"জেনেটের হাঁসগুলি প্রতিদিন 16 টি করে ডিম পাড়ে।\"\n","english_sentence = \"Janet's ducks lay 16 eggs every day.\"\n","\n","# Convert sentences to ids\n","bangla_ids = sentence_to_ids(bangla_sentence, bangla_vocab)\n","english_ids = sentence_to_ids(english_sentence, english_vocab)\n","\n","print(\"Bangla IDs:\", bangla_ids)\n","print(\"English IDs:\", english_ids)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"X5V7MhyveI9K"},"outputs":[],"source":["import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, src_file, tgt_file, src_vocab, tgt_vocab, max_len=50):\n","        with open(src_file, encoding=\"utf-8\") as f:\n","            self.src_sentences = f.readlines()\n","        with open(tgt_file, encoding=\"utf-8\") as f:\n","            self.tgt_sentences = f.readlines()\n","        self.src_vocab = src_vocab\n","        self.tgt_vocab = tgt_vocab\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.src_sentences)\n","\n","    def __getitem__(self, idx):\n","        src_ids = sentence_to_ids(self.src_sentences[idx].strip(), self.src_vocab, self.max_len)\n","        tgt_ids = sentence_to_ids(self.tgt_sentences[idx].strip(), self.tgt_vocab, self.max_len)\n","        return torch.tensor(src_ids), torch.tensor(tgt_ids)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Define the custom collate function\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src, tgt in batch:\n","        src_batch.append(src)\n","        tgt_batch.append(tgt)\n","    \n","    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n","    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n","    \n","    return src_batch, tgt_batch"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"zwJQWBwTeO56"},"outputs":[],"source":["# =============================\n","# Training and Evaluation\n","# =============================\n","\n","def train_model(model, dataloader, optimizer, criterion, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","        for i, (src, tgt) in enumerate(dataloader):\n","            src, tgt = src.to(device), tgt.to(device)\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","\n","            optimizer.zero_grad()\n","            output = model(src, tgt_input)\n","\n","            # Fix: Use reshape instead of view\n","            loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n","\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader)}\")\n","\n","\n","\n","def evaluate_model(model, dataloader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for src, tgt in dataloader:\n","            src, tgt = src.to(device), tgt.to(device)\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","\n","            output = model(src, tgt_input)\n","            loss = criterion(output.view(-1, output.shape[-1]), tgt_output.view(-1))\n","            total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def translate_sentence(model, src_sentence, src_vocab, tgt_vocab, max_len=50, device=\"cpu\"):\n","    model.eval()\n","    src_ids = sentence_to_ids(src_sentence, src_vocab, max_len)\n","    src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n","    tgt_ids = [tgt_vocab[\"<sos>\"]]\n","\n","    for _ in range(max_len):\n","        tgt_tensor = torch.tensor(tgt_ids).unsqueeze(0).to(device)\n","        output = model(src_tensor, tgt_tensor)\n","        next_token = output.argmax(2)[:, -1].item()\n","        tgt_ids.append(next_token)\n","        if next_token == tgt_vocab[\"<eos>\"]:\n","            break\n","\n","    return \" \".join([key for key, val in tgt_vocab.items() if val in tgt_ids])\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216424,"status":"ok","timestamp":1736404695169,"user":{"displayName":"Rahul Drabit Chowdhury","userId":"16125043446260055594"},"user_tz":-360},"id":"_SaWhu4qd13x","outputId":"00744cc9-a441-41e3-e22b-faa94dae4fb6"},"outputs":[],"source":["# =============================\n","# Main Execution\n","# =============================\n","\n","if __name__ == \"__main__\":\n","    # Build vocabularies\n","    src_vocab = build_vocab(\"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/bn_data.txt\")\n","    tgt_vocab = build_vocab(\"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/en_data.txt\")\n","\n","    # Save vocabularies (optional)\n","    with open(\"src_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n","        json.dump(src_vocab, f, indent=4)\n","    with open(\"tgt_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n","        json.dump(tgt_vocab, f, indent=4)\n","\n","    # Create dataset and dataloader\n","    dataset = TranslationDataset(\"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/bn_data.txt\", \"/teamspace/studios/this_studio/Attention-Is-All-You-Need/data/en_data.txt\", src_vocab, tgt_vocab)\n","    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, \n","                 num_decoder_layers, dim_feedforward, max_len=5000):\n","        super(TransformerModel, self).__init__()\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_len)\n","        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n","        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def generate_mask(self, src, tgt):\n","        src_seq_len = src.shape[1]\n","        tgt_seq_len = tgt.shape[1]\n","        src_mask = self.transformer.generate_square_subsequent_mask(src_seq_len).to(src.device)\n","        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n","        return src_mask, tgt_mask\n","\n","    def forward(self, src, tgt):\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","        enc_output = self.transformer.encoder(src_embedded, src_mask)\n","        dec_output = self.transformer.decoder(tgt_embedded, enc_output, tgt_mask)\n","        output = self.fc_out(dec_output)\n","        return output"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Initialize model\n","model = TransformerModel(\n","    src_vocab_size=len(src_vocab),\n","    tgt_vocab_size=len(tgt_vocab),\n","    d_model=512,\n","    nhead=8,\n","    num_encoder_layers=6,\n","    num_decoder_layers=6,\n","    dim_feedforward=2048,\n",")\n","## Define optimizer and loss function\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n","\n","# Train model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"The shape of the 2D attn_mask is torch.Size([52, 52]), but should be (32, 32).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_translation.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Fix: Use reshape instead of view\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","Cell \u001b[0;32mIn[24], line 26\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     24\u001b[0m src_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding(src)))\n\u001b[1;32m     25\u001b[0m tgt_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding(tgt)))\n\u001b[0;32m---> 26\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdecoder(tgt_embedded, enc_output, tgt_mask)\n\u001b[1;32m     28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(dec_output)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    903\u001b[0m         x\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     )\n\u001b[1;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/functional.py:6131\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6129\u001b[0m     correct_2d_size \u001b[38;5;241m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 6131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   6132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 2D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_2d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6133\u001b[0m         )\n\u001b[1;32m   6134\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   6135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n","\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([52, 52]), but should be (32, 32)."]}],"source":["train_model(model, dataloader, optimizer, criterion, num_epochs=2, device=device)\n","\n","# Save model\n","torch.save(model.state_dict(), \"transformer_translation.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch 0:\n","Source: tensor([[   2,    1,    1,  ...,    0,    0,    0],\n","        [   2,    1,   84,  ...,    0,    0,    0],\n","        [   2, 1148,  221,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 1272,    1,  ...,    0,    0,    0],\n","        [   2,   99,   72,  ...,    0,    0,    0],\n","        [   2,  824,  456,  ...,    0,    0,    0]])\n","Target: tensor([[   2,  185,  930,  ...,    0,    0,    0],\n","        [   2,    1,  534,  ...,    0,    0,    0],\n","        [   2, 1033,   16,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 1145,  477,  ...,    0,    0,    0],\n","        [   2,  100,  101,  ...,    0,    0,    0],\n","        [   2,  756,  116,  ...,    0,    0,    0]])\n","Batch 1:\n","Source: tensor([[   2,    1,  526,  ...,    0,    0,    0],\n","        [   2,    1,   15,  ...,    0,    0,    0],\n","        [   2,  484,  857,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 1266, 1267,  ...,   98,    3,    0],\n","        [   2,  620,  112,  ...,    0,    0,    0],\n","        [   2,    1,   38,  ...,    0,    0,    0]])\n","Target: tensor([[   2,    1,  101,  ...,   62,   34,    3],\n","        [   2,  398,  180,  ...,   23,  430,    3],\n","        [   2,  727,  781,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 1139,   98,  ...,   93,  221,    3],\n","        [   2,  581,  116,  ...,    0,    0,    0],\n","        [   2,    1,  783,  ...,    0,    0,    0]])\n","Batch 2:\n","Source: tensor([[   2,  267,  268,  ...,    0,    0,    0],\n","        [   2,    1,   34,  ...,    0,    0,    0],\n","        [   2,   31, 1012,  ...,  567,  581,    3],\n","        ...,\n","        [   2,   31,    1,  ...,    0,    0,    0],\n","        [   2,    1,  750,  ...,    0,    0,    0],\n","        [   2,    1,    1,  ...,    0,    0,    0]])\n","Target: tensor([[   2,  266,    1,  ...,    0,    0,    0],\n","        [   2,    1,  150,  ...,    0,    0,    0],\n","        [   2,   36,  918,  ..., 1178,  875,    3],\n","        ...,\n","        [   2,   86,   50,  ...,    0,    0,    0],\n","        [   2,    1,   98,  ...,   23,  299,    3],\n","        [   2,    1,  279,  ...,    0,    0,    0]])\n","Batch 3:\n","Source: tensor([[  2, 517, 336,  ...,   0,   0,   0],\n","        [  2,  60,  61,  ...,   0,   0,   0],\n","        [  2,   1,  23,  ..., 421, 581,   3],\n","        ...,\n","        [  2,   1,  72,  ..., 771,   3,   0],\n","        [  2,   1,   4,  ...,   0,   0,   0],\n","        [  2,  31,   1,  ...,   0,   0,   0]])\n","Target: tensor([[  2, 484, 114,  ...,   3,   0,   0],\n","        [  2,  65,  48,  ...,   0,   0,   0],\n","        [  2,   1, 832,  ...,  50, 841,   3],\n","        ...,\n","        [  2,   1,  98,  ...,  97,  34,   3],\n","        [  2,   1, 291,  ...,   0,   0,   0],\n","        [  2, 398, 180,  ..., 638,   1,   3]])\n","Batch 4:\n","Source: tensor([[   2,  434,   54,  ...,  161,  981,    3],\n","        [   2,   23,   62,  ...,    0,    0,    0],\n","        [   2,    1,    1,  ...,    0,    0,    0],\n","        ...,\n","        [   2,   31,  961,  ...,    0,    0,    0],\n","        [   2,  327,  328,  ...,    0,    0,    0],\n","        [   2, 1043,   38,  ...,    0,    0,    0]])\n","Target: tensor([[  2, 406, 188,  ...,  74,   3,   0],\n","        [  2,   1, 167,  ...,   0,   0,   0],\n","        [  2,   1, 151,  ...,   0,   0,   0],\n","        ...,\n","        [  2, 891, 248,  ...,   0,   0,   0],\n","        [  2, 321,  22,  ...,   0,   0,   0],\n","        [  2, 951,  53,  ...,   0,   0,   0]])\n","Batch 5:\n","Source: tensor([[   2,    1,   63,  ...,    0,    0,    0],\n","        [   2,    1,   31,  ...,    0,    0,    0],\n","        [   2,    1,    1,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 1096,  221,  ...,  143, 1098,    3],\n","        [   2,  271, 1136,  ...,    0,    0,    0],\n","        [   2, 1215,    1,  ...,    0,    0,    0]])\n","Target: tensor([[   2,    1,  127,  ...,    0,    0,    0],\n","        [   2,    1,   48,  ...,    0,    0,    0],\n","        [   2,  793,   98,  ...,    0,    0,    0],\n","        ...,\n","        [   2,  990,   16,  ...,  136,   23,    3],\n","        [   2, 1024,  101,  ...,    0,    0,    0],\n","        [   2, 1089,   98,  ...,    0,    0,    0]])\n","Batch 6:\n","Source: tensor([[   2, 1061,    4,  ...,    0,    0,    0],\n","        [   2,    1,  490,  ..., 1042,  109,    3],\n","        [   2,   31,    1,  ...,    0,    0,    0],\n","        ...,\n","        [   2,  141,    1,  ...,    0,    0,    0],\n","        [   2,    1, 1296,  ...,    3,    0,    0],\n","        [   2,  881,    1,  ...,    0,    0,    0]])\n","Target: tensor([[  2, 963, 407,  ...,   0,   0,   0],\n","        [  2,   1, 340,  ...,  52, 116,   3],\n","        [  2,  86,  50,  ...,   0,   0,   0],\n","        ...,\n","        [  2, 138,   1,  ...,  62,  63,   3],\n","        [  2,   1, 414,  ..., 985, 136,   3],\n","        [  2,   1, 777,  ...,  23,  74,   3]])\n","Batch 7:\n","Source: tensor([[  2,   1,   1,  ...,   0,   0,   0],\n","        [  2,   1,   1,  ...,   0,   0,   0],\n","        [  2, 141,  63,  ...,   0,   0,   0],\n","        ...,\n","        [  2,  61, 223,  ...,   0,   0,   0],\n","        [  2,   1,  31,  ...,   0,   0,   0],\n","        [  2,  31,   1,  ...,   0,   0,   0]])\n","Target: tensor([[   2,  185,    1,  ...,    3,    0,    0],\n","        [   2,    1,    1,  ...,    0,    0,    0],\n","        [   2,  138, 1108,  ...,    0,    0,    0],\n","        ...,\n","        [   2,  939,  530,  ...,    0,    0,    0],\n","        [   2,  539,  116,  ...,    0,    0,    0],\n","        [   2,   36,  606,  ...,    0,    0,    0]])\n"]}],"source":["\n","\n","\n","\n","\n","\n","# # Example loop to access the DataLoader\n","# for i, (src, tgt) in enumerate(dataloader):\n","#     print(f\"Batch {i}:\")\n","#     print(\"Source:\", src)\n","#     print(\"Target:\", tgt)"]},{"cell_type":"markdown","metadata":{"id":"K-XM9JpwymSh"},"source":["# Testing Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1736404943314,"user":{"displayName":"Rahul Drabit Chowdhury","userId":"16125043446260055594"},"user_tz":-360},"id":"0P7AdJhFylw8","outputId":"7745cc91-fc16-4af3-d81c-5a300c965aaa"},"outputs":[],"source":["# Re-initialize the model architecture\n","#model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n","\n","# Load the trained weights\n","model.load_state_dict(torch.load('transformer_translation.pth'))\n","model.eval()  # Set the model to evaluation mode\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6t9tHIKDyysV"},"outputs":[],"source":["test_sentences = [\n","    \"তাঁর বাবার নাম হাউস সরদার এবং মায়ের নাম জরিমন নেছা\",\n","    \"তাঁর স্ত্রীর নাম আনোয়ারা খাতুন ওরফে আনারজান\"\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VdVgqVdByxkg"},"outputs":[],"source":["def preprocess_input(sentence, src_vocab):\n","    # Tokenize the sentence (you may use a tokenizer like word_tokenize or simple split)\n","    sentence_tokens = sentence.split()  # Basic split (you can improve this with a more sophisticated tokenizer)\n","    # Convert tokens to their corresponding word IDs using the vocabulary\n","    sentence_ids = [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in sentence_tokens]\n","    return sentence_ids\n","\n","# Preprocess the test sentences\n","test_input_ids = [preprocess_input(sentence, src_vocab) for sentence in test_sentences]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rivz_-lvzSiF"},"outputs":[],"source":["# Create reverse vocab for tgt_vocab (so we can decode from word IDs back to words)\n","reverse_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390,"status":"ok","timestamp":1736404980125,"user":{"displayName":"Rahul Drabit Chowdhury","userId":"16125043446260055594"},"user_tz":-360},"id":"mmvPCRO5y7qS","outputId":"4c25c85e-b926-444b-c0a3-94ed8b669c6a"},"outputs":[],"source":["def translate_sentence(src, model, max_length=50):\n","    # Convert the input sentence to a tensor (batch_size = 1)\n","    src_tensor = torch.tensor(src).unsqueeze(0).to(device)\n","\n","    # Create mask for the source sequence\n","    src_mask = (src_tensor != src_vocab[\"<pad>\"]).unsqueeze(1).unsqueeze(2).to(device)\n","\n","    # Decoder input (start with <sos> token)\n","    tgt_input = torch.tensor([tgt_vocab[\"<sos>\"]]).unsqueeze(0).to(device)\n","\n","    # Store the output sequence\n","    translated_sentence = []\n","\n","    # Iterate to generate the translated sentence token by token\n","    for _ in range(max_length):\n","        output = model(src_tensor, tgt_input)  # Forward pass through the model\n","        output_token = output.argmax(dim=-1)[:, -1].item()  # Get the predicted token (last in the sequence)\n","\n","        if output_token == tgt_vocab[\"<eos>\"]:\n","            break  # Stop when <eos> is generated\n","\n","        translated_sentence.append(output_token)\n","\n","        # Update tgt_input for the next token (append the predicted token)\n","        tgt_input = torch.cat((tgt_input, torch.tensor([[output_token]]).to(device)), dim=-1)\n","\n","    # Convert output tokens back to words\n","    translated_words = [reverse_tgt_vocab[token] for token in translated_sentence]\n","    return \" \".join(translated_words)\n","\n","# Translate the test sentences\n","for sentence in test_sentences:\n","    input_ids = preprocess_input(sentence, src_vocab)\n","    translation = translate_sentence(input_ids, model)\n","    print(f\"Bangla: {sentence}\")\n","    print(f\"English: {translation}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPHoC9__ziFF"},"outputs":[],"source":["samples = [\n","    \"এলোমেলোভাবে তুলে নেওয়া প্রথম সঠিক উত্তরদাতাকেই বিজয়ী ঘোষণা করা হবে।\",\n","    \"আমার সোনার বাংলা, আমি তোমায় ভালোবাস।\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1736405263586,"user":{"displayName":"Rahul Drabit Chowdhury","userId":"16125043446260055594"},"user_tz":-360},"id":"6RXjAAwYziBw","outputId":"181e0b7b-5fb1-406c-a780-2cb9bd6d8357"},"outputs":[],"source":["for s in samples:\n","    input_ids = preprocess_input(s, src_vocab)\n","    translation = translate_sentence(input_ids, model)\n","    print(f\"Bangla: {s}\")\n","    print(f\"English: {translation}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rydx9tbLzh_K"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fYLdBlnzh8e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nLGmh72zh52"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQSeHzmgzh3R"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1d2CcP-ybfHGLDrQU0mrtiJVE_1PjXtVT","timestamp":1736405400313}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
