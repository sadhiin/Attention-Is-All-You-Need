{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, dk, dv = 4, 8, 8\n",
    "q = np.random.randn(L, dk)\n",
    "k = np.random.randn(L, dk)\n",
    "v = np.random.randn(L, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  [[ 1.16797418 -0.73724956  0.75041732 -0.1220419   1.70315355  0.31347885\n",
      "   0.34485802  0.4460984 ]\n",
      " [ 3.30107262  0.52084263 -0.30178335  1.6757843  -0.5155369  -2.57282086\n",
      "   0.7492196  -0.1116739 ]\n",
      " [ 1.79346332  1.43356091  1.04335097 -0.17879843  0.64886601 -1.36614847\n",
      "   0.23380491  0.87536806]\n",
      " [ 1.47103375 -1.58489204 -0.45911092  0.28391595  0.21250504 -1.61590866\n",
      "  -1.29484902  0.00475083]] \n",
      "\n",
      "\n",
      "K:  [[-0.27718816 -1.65065268  0.65895024 -0.30877976  0.08718337 -1.25869869\n",
      "  -0.189555    0.19835477]\n",
      " [ 0.34020432  0.52734301  0.70054018  0.02666352 -0.47575991  0.27451574\n",
      "   0.31178952 -0.80116187]\n",
      " [-0.32764932 -0.14549352 -0.40258405  0.75537362  1.05953459 -1.34530308\n",
      "   1.99241804  0.17674986]\n",
      " [-1.40543092  0.43389184 -0.07028518  0.10003431  1.26343057 -2.0707861\n",
      "   0.51659185 -1.03069964]] \n",
      "\n",
      "\n",
      "V:  [[-1.20455614 -0.78141984 -0.60550966 -0.49607676  0.05358656  0.31648808\n",
      "   1.97222775  0.97455048]\n",
      " [-0.22988732 -2.3585375   0.49206019 -0.74539263 -0.530485   -0.23619713\n",
      "   0.80175571  0.96790588]\n",
      " [-1.20769139 -1.33242785 -1.59215534  2.60984511  0.33729297  0.46552584\n",
      "   1.51418349  0.72913429]\n",
      " [ 0.5936924  -0.6305729  -0.95026899  0.87634056 -1.56283632  0.3099347\n",
      "  -0.36928654  1.2784266 ]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q: \", q, '\\n\\n')\n",
    "print(\"K: \", k, '\\n\\n')\n",
    "print(\"V: \", v, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20239352, -0.44310134,  1.479061  , -0.80531921],\n",
       "       [ 0.53823363,  1.09303249,  4.61797795,  0.95396625],\n",
       "       [-0.21525807,  0.78011689,  1.79464107,  0.87754205],\n",
       "       [ 4.11701088, -1.60160544, -0.03209583,  0.24644341]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.3772888688135403),\n",
       " np.float64(0.7481376289021062),\n",
       " np.float64(1.2000978210749365))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), v.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.443300481459231)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are calculating the variance of the dot product of q and k which is ....\n",
    "# larger then the variance of q and k. To reduce the variance of the dot product we can scale the q and k by 1/sqrt(dk)\n",
    "\n",
    "np.matmul(q, k.T).var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.3772888688135403),\n",
       " np.float64(0.7481376289021062),\n",
       " np.float64(0.30541256018240387))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(dk)\n",
    "\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42511031, -0.15665998,  0.52292703, -0.28472334],\n",
       "       [ 0.19029433,  0.38644534,  1.63270176,  0.337278  ],\n",
       "       [-0.07610522,  0.27581297,  0.63450144,  0.31025797],\n",
       "       [ 1.45558315, -0.56625303, -0.01134759,  0.0871309 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "\n",
    "- A process to hide the future information from model durning the training process\n",
    "- Upper diagonal would be `zero`, with will become `inf` drurning the `softmax` calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., -inf, -inf, -inf],\n",
       "       [  1.,   1., -inf, -inf],\n",
       "       [  1.,   1.,   1., -inf],\n",
       "       [  1.,   1.,   1.,   1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.inf\n",
    "# this will help the model to see the future tokens in the sequence\n",
    "(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.42511031,       -inf,       -inf,       -inf],\n",
       "       [1.19029433, 1.38644534,       -inf,       -inf],\n",
       "       [0.92389478, 1.27581297, 1.63450144,       -inf],\n",
       "       [2.45558315, 0.43374697, 0.98865241, 1.0871309 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fn(x) -> np.ndarray:\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.45111887, 0.54888113, 0.        , 0.        ],\n",
       "       [0.22436527, 0.31900079, 0.45663394, 0.        ],\n",
       "       [0.61822085, 0.08185993, 0.14258168, 0.15733755]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax_fn(scaled + mask)  # (L, L)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.20455614, -0.78141984, -0.60550966, -0.49607676,  0.05358656,\n",
       "         0.31648808,  1.97222775,  0.97455048],\n",
       "       [-0.66957881, -1.64706996, -0.00307428, -0.63292153, -0.2669993 ,\n",
       "         0.0131296 ,  1.32977774,  0.97090339],\n",
       "       [-0.89506767, -1.53613058, -0.70591991,  0.84266061, -0.00318276,\n",
       "         0.20823676,  1.38968769,  0.86036549],\n",
       "       [-0.84228483, -0.96535234, -0.71058386,  0.14229459, -0.20809817,\n",
       "         0.29146427,  1.44269616,  0.98682583]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.20455614, -0.78141984, -0.60550966, -0.49607676,  0.05358656,\n",
       "         0.31648808,  1.97222775,  0.97455048],\n",
       "       [-0.22988732, -2.3585375 ,  0.49206019, -0.74539263, -0.530485  ,\n",
       "        -0.23619713,  0.80175571,  0.96790588],\n",
       "       [-1.20769139, -1.33242785, -1.59215534,  2.60984511,  0.33729297,\n",
       "         0.46552584,  1.51418349,  0.72913429],\n",
       "       [ 0.5936924 , -0.6305729 , -0.95026899,  0.87634056, -1.56283632,\n",
       "         0.3099347 , -0.36928654,  1.2784266 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
